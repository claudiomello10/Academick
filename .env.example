# ===========================================
# AcademiCK Environment Configuration
# ===========================================
# Copy this file to .env and fill in your values
#
# QUICK START:
# 1. Set at least one LLM API key (OPENAI_API_KEY, ANTHROPIC_API_KEY, or DEEPSEEK_API_KEY)
# 2. Set all required passwords (POSTGRES_PASSWORD, REDIS_PASSWORD, SESSION_SECRET,
#    ADMIN_PASSWORD, GUEST_PASSWORD) - the system will not start without them
#
# CONFIGURATION NOTES:
# - Commented variables (#) show optional settings with defaults
# - Uncomment and modify as needed for your deployment
# - Most services have sensible defaults for development
# - See "ADVANCED CONFIGURATION" section at the bottom for advanced options
#

# ===========================================
# PostgreSQL Database
# ===========================================
# Database connection credentials
POSTGRES_USER=academick
POSTGRES_PASSWORD=
POSTGRES_DB=academick

# Full connection string (automatically constructed from above)
# Used by: api-gateway, pdf-service, pdf-worker
# DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}

# ===========================================
# Redis Cache & Task Queue
# ===========================================
# Redis authentication password
# IMPORTANT: Change this to a secure random password in production!
REDIS_PASSWORD=

# Redis connection URL for session caching and Celery task queue
# Used by: api-gateway, pdf-service, pdf-worker
# REDIS_URL is constructed automatically in docker-compose.yml from REDIS_PASSWORD

# ===========================================
# Qdrant Vector Database
# ===========================================
# Qdrant hostname (use 'qdrant' for Docker, 'localhost' for local dev)
QDRANT_HOST=qdrant
# Qdrant HTTP API port
QDRANT_PORT=6333
# Collection name for storing embeddings
QDRANT_COLLECTION=academick_embeddings

# ===========================================
# Service URLs (Internal Docker Network)
# ===========================================
# Embedding service endpoint (BGE-M3 model)
EMBEDDING_SERVICE_URL=http://embedding-service:8002
# Intent classification service endpoint
INTENT_SERVICE_URL=http://intent-service:8001

# ===========================================
# LLM API Keys & Configuration
# ===========================================
# At least ONE of these API keys is required for the RAG system to work.
# You only need to provide the API key for the provider you plan to use.
#
# For example:
# - If only using OpenAI models → only set OPENAI_API_KEY
# - If only using Anthropic models → only set ANTHROPIC_API_KEY
# - If only using DeepSeek models → only set DEEPSEEK_API_KEY

# OpenAI API key (get from: https://platform.openai.com/api-keys)
# Example models gpt-5-nano gpt-5-mini etc. (check OpenAI docs for latest models)
OPENAI_API_KEY=

# Anthropic API key (get from: https://console.anthropic.com/)
# Example models: claude-4-5 haiku-4-5 opus-4-6 etc. (check Anthropic docs for latest models)
ANTHROPIC_API_KEY=

# DeepSeek API key (get from: https://platform.deepseek.com/)
# Example model: deepseek-chat (check DeepSeek docs for latest models)
DEEPSEEK_API_KEY=

# Default LLM model for RAG responses
# RECOMMENDED: Use the latest models for best quality
#   - OpenAI: gpt-5-nano (best quality), gpt-5-mini (fast & cost-effective)
#   - Anthropic: claude-4-5 (newest), haiku-4-5, opus-4-6
#   - DeepSeek: deepseek-chat
# Used by: api-gateway
# DEFAULT_MODEL=gpt-5-mini

# Model used for query enhancement (generating focused search queries from user input)
# This should be a fast, cheap model since it runs on every query before the main LLM
# QUERY_ENHANCEMENT_MODEL=gpt-5-nano

# ===========================================
# Project Subject Configuration
# ===========================================
# The default academic subject for new sessions.
# This determines the context and behavior of the AI assistant.
# Examples: "Machine Learning", "Data Structures", "Linear Algebra",
#           "Organic Chemistry", "World History", "Constitutional Law"
DEFAULT_SUBJECT=Machine Learning

# ===========================================
# Session Configuration
# ===========================================
# Session expiration time in minutes (30 minutes default)
SESSION_TTL_MINUTES=30
# Secret key for session token encryption
# IMPORTANT: Change this to a secure random string in production!
SESSION_SECRET=

# ===========================================
# API Documentation
# ===========================================
# Enable Swagger UI and ReDoc documentation endpoints (/docs, /redoc, /openapi.json)
# Set to false in production to hide API documentation
# NOTE: Even when true, nginx blocks these routes externally as defense-in-depth.
# Access docs internally via: docker compose exec api-gateway curl http://localhost:8000/docs
DOCS_ENABLED=true

# ===========================================
# Config Users (Testing)
# ===========================================
# Enable hardcoded test users (admin/guest)
# IMPORTANT: Set to false in production for security
CONFIG_USERS_ENABLED=true
# Admin user password (username: admin)
ADMIN_PASSWORD=
# Guest user password (username: guest)
GUEST_PASSWORD=

# ===========================================
# Admin Dashboard Features
# ===========================================
# Enable/disable admin dashboard features
# IMPORTANT: Set to false to hide features in production if not needed

# Show Qdrant snapshot management buttons (load/download snapshots)
# Default: true
ENABLE_SNAPSHOT_MANAGEMENT=true

# Show PDF upload button for processing new books
# Default: true
ENABLE_PDF_UPLOAD=true

# ===========================================
# Embedding Service Configuration
# ===========================================
# Device to run embedding model on (cpu or gpu)
# Default: gpu (automatically falls back to cpu if CUDA unavailable)
# Set to "cpu" to force CPU inference even on GPU-enabled systems
EMBEDDING_DEVICE=gpu

# Batch size for embedding generation (higher = faster but more memory)
# Range: 1-32, Default: 16
# Safe to adjust based on your GPU/CPU memory
EMBEDDING_BATCH_SIZE=16

# ===========================================
# Frontend Configuration
# ===========================================
# API Gateway URL (used by Next.js frontend at build time)
# For Docker: http://localhost (through nginx reverse proxy)
# For production: http://your-domain.com
NEXT_PUBLIC_API_URL=http://localhost

# Default subject displayed in the frontend (should match DEFAULT_SUBJECT)
NEXT_PUBLIC_DEFAULT_SUBJECT=Machine Learning

# ===========================================
# Optional: Development & Debugging
# ===========================================
# Enable debug mode (verbose logging)
# DEBUG=true

# Log level (debug, info, warning, error, critical)
# LOG_LEVEL=debug

# Enable SQL query logging (PostgreSQL)
# POSTGRES_ECHO=false

# ===========================================
# ADVANCED CONFIGURATION
# ===========================================
# ⚠️  WARNING: Only modify these if you know what you're doing!
# ⚠️  Changing these values incorrectly can break the system.
# ⚠️  Most users should leave these at their defaults.
# ===========================================

# --- Nginx Advanced ---
# Maximum upload size for file uploads (PDFs, snapshots)
# ⚠️ WARNING: Set in config/nginx.conf (client_max_body_size), not here. Requires container restart
# NGINX_MAX_BODY_SIZE=500m

# --- Redis Advanced ---
# Maximum memory allocation (512mb recommended for production)
# WARNING: Set in docker-compose.yml, not here. Requires container restart
# REDIS_MAXMEMORY=512mb

# Eviction policy when memory limit is reached
# WARNING: Set in docker-compose.yml, not here. Requires container restart
# REDIS_MAXMEMORY_POLICY=allkeys-lru

# Redis database number (0-15)
# Default: 0
# REDIS_DB=0

# --- Qdrant Advanced ---
# Qdrant internal ports (configured in docker-compose.yml)
# WARNING: Changing these requires updating docker-compose.yml
# QDRANT__SERVICE__GRPC_PORT=6334
# QDRANT__SERVICE__HTTP_PORT=6333

# Qdrant snapshots directory for backup/restore
# WARNING: Changing this requires updating docker-compose.yml volume mounts
# QDRANT_SNAPSHOTS_DIR=./data/qdrant_snapshots

# --- Embedding Service Advanced ---
# HuggingFace model name for embeddings
# WARNING: Changing this requires re-processing all PDFs and re-creating the Qdrant collection
# Default: BAAI/bge-m3 (1024-dim dense + sparse vectors)
# MODEL_NAME=BAAI/bge-m3

# GPU device IDs for CUDA (comma-separated for multi-GPU)
# WARNING: Only applies when EMBEDDING_DEVICE=gpu
# Default: 0 (first GPU)
# CUDA_VISIBLE_DEVICES=0

# Maximum input length for tokenization (in tokens)
# WARNING: Must match the model's max context length
# Default: 8192 (BGE-M3 max context length)
# MAX_LENGTH=8192

# Enable FP16 precision for GPU acceleration (reduces memory, slightly faster)
# WARNING: Only applies when EMBEDDING_DEVICE=gpu, ignored on CPU
# Default: true (requires GPU)
# USE_FP16=true

# --- Intent Classification Service Advanced ---
# HuggingFace model name for intent classification
# WARNING: Changing this requires a compatible model trained for 4-class classification
# Default: claudiomello/AcademiCK-intent-classifier
# INTENT_MODEL_NAME=claudiomello/AcademiCK-intent-classifier

# Device to run intent classifier on (cpu or cuda)
# Default: cpu (this is a lightweight model)
# INTENT_DEVICE=cpu

# --- PDF Processing Service Advanced ---
# Maximum PDF upload file size in megabytes
# Default: 100 (100MB)
# MAX_UPLOAD_SIZE_MB=100

# Text chunk size in characters (affects retrieval granularity)
# WARNING: Changing this requires re-processing all PDFs
# Default: 3000 chars (~600 words)
# CHUNK_SIZE=3000

# Character overlap between chunks (prevents information loss at boundaries)
# WARNING: Must be less than CHUNK_SIZE
# Default: 1000 chars (~200 words)
# CHUNK_OVERLAP=1000

# Minimum chunk length to keep (filters out very short chunks)
# Default: 300 chars (~60 words)
# MIN_CHUNK_LENGTH=300

# Upload directory for PDF files (inside container)
# WARNING: Changing this requires updating docker-compose.yml volume mounts
# Default: /app/processed/uploads
# UPLOAD_DIR=/app/processed/uploads

# Processed files directory (inside container)
# WARNING: Changing this requires updating docker-compose.yml volume mounts
# Default: /app/processed
# PROCESSED_DIR=/app/processed

# Celery worker concurrency (number of parallel PDF processing tasks)
# WARNING: Higher values use more memory. Adjust based on available CPU cores
# Default: 2 (set in docker-compose.yml)
# CELERY_WORKER_CONCURRENCY=2
